---
title: "COMPSCIX 415.2 Homework 9/Final"
author: "Shannon Hamilton"
date: "4/3/2018"
output: 
  html_document: 
    toc: yes
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)
library(modelr)
library(tidyverse)
library(rpart)
library(partykit)
library(randomForest)
library(ROCR)
```

##Bootstrapping

###Question 1

```{r}
file_path <- '/Users/shamilto/Downloads/train-f.csv'
train <-read_csv(file_path,col_names=TRUE)
train <- as.tibble(train)
train$Survived <- as.factor(train$Survived)
train$Name <- as.factor(train$Name)
train$Sex <- as.factor(train$Sex)
train$Ticket <- as.factor(train$Ticket)
train$Cabin <- as.factor(train$Cabin)
train$Embarked <- as.factor(train$Embarked)
glimpse(train)

```

###Question 2

```{r}
titanic_boot <- bootstrap(data = train, n = 100)
glimpse(titanic_boot)
```

<p> Confirmed the above created a tibble by using "glimpse". </p>

###Question 3

```{r}
# since the strap column of titanic_boot is a list, we can
# extract the resampled data using the double brackets [[]],
# and just pick out a few of them to compare the number of
# distinct rows
as.tibble(titanic_boot$strap[[1]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[2]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[3]]) %>% n_distinct()

```

We see there are different number of distinct values accross the different bootstrap samples because of the repacement.

###Question 4

```{r}
age_mean <- function(data) {
data <- as.tibble(data) # convert input data set to a tibble
mean_age <- mean(data$Age, na.rm = TRUE) # take the mean of Age, remove NAs
return(mean_age) # return the mean value of Age from data
}
# loop through the 100 bootstrap samples and use the age_mean()
# function
all_means <- rep(NA, 100)

# start the loop
for(i in 1:100) {
all_means[i] <- age_mean(titanic_boot$strap[[i]])
}
# take a look at some of the means you calculated from your samples
head(all_means)

# convert to a tibble so we can use if for plotting
all_means <- tibble(all_means = all_means)

```

<p> We should see a normal distribution of these means when we graph them. </p>

###Question 5

```{r}

glimpse(all_means)
           
ggplot(data=all_means, aes(all_means))+
geom_histogram(binwidth = 0.02)

```
<p> We see an approximately normal distribution of the means as expected. If we used a larger number of repititions, this graph would continue to approach normal. </p>


###Question 6


```{r}
#theoretical SE

(SE <- (sd(train$Age, na.rm = TRUE))/(sqrt(891)))

```


```{r}
#Bootstrapped

(SE <- (sd(all_means$all_means, na.rm = TRUE))/(sqrt(100)))

```

<p>The standard error of the sample means is much lower. </p>

##Random Forest

###Question 1

```{r}

set.seed(987)
model_data <- resample_partition(train, c(test = 0.3, train = 0.7))

train_set <- as.tibble(model_data$train)
test_set <- as.tibble(model_data$test)

```

###Question 2

```{r}

tree_mod <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_set)
plot(as.party(tree_mod))

```

<p> Even though we are including more features, sex is still the #1 split. Age is an important feature for men especially, which we didn't include in our tree last class. </p>

###Question 3

```{r}

rf_mod <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train_set,
ntrees = 500,
mtry = 4,
na.action = na.roughfix)

```

###Question 4

```{r}

library(randomForest)
library(ROCR)

rf_preds <- predict(rf_mod, newdata = test_set, type = 'prob')[,2]
tree_preds <- predict(tree_mod, newdata = test_set)[,2]
pred_rf <- prediction(predictions = rf_preds, labels = test_set$Survived)
pred_tree <- prediction(predictions = tree_preds, labels = test_set$Survived)


perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])
names(perf_tree_tbl) <- c('fpr', 'tpr')

perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])
names(perf_rf_tbl) <- c('fpr', 'tpr')

perf_tree_auc <- performance(pred_tree, measure = 'auc')
perf_tree_auc@y.values

perf_rf_auc <- performance(pred_rf, measure = 'auc')
perf_rf_auc@y.values

```


<p> Random Forest has a higher AUC than the single decision tree. </p>

###Question 5

```{r}
plot_roc <- function(perf_tbl) {
p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
geom_line(color = 'blue') +
geom_abline(intercept = 0, slope = 1, lty = 3) +
labs(x = 'False positive rate', y = 'True positive rate') +
theme_bw()
return(p)
}

plot_roc(perf_rf_tbl)

plot_roc(perf_tree_tbl)
```


###Question 6


which model performs better: decision tree or random forest?
â€¢ what is the approximate false positive rate, for both the decision tree and the random forest, if we
attain a true positive rate of approximately 0.75? Answers do not need to be exact - just ballpark it by
looking at the plots.







